---
layout: post
title:  强化学习笔记10-新手村毕业总结
categories: [Reinforcement-Learning]
---


经过了4个多月的时间，终于系统的学习完了Daivd Silver的课程，下面简要总结相关内容以及他们的内在联系。

最开始，定义了强化学习问题---智能体动态的与环境交互，通过奖励反馈不断优化自己的行为，以期望不断适应环境变化，得到长期的最优效果。（第1讲）

经过抽象和简化，将强化学习抽象为马尔科夫过程MDP。MDP由$<S,A,P,R,\lambda>$组成，其中状态集合S，动作集合A由用户定义，超参数$\lambda$需要调试。MDP可以通过贝尔曼方程进行递归表示，并且定义了长期回报的递归形式。（第2讲）   

在强化学习语境下，称P和R为模型model。如果model已知，那么可以使用Dynamic Progamming求解RL问题，得到最终的策略$\pi$。求解过程由策略评估和策略改进的循环，可以抽象为通用策略迭代（GPI）过程。（第3讲）

但是到多数情况下，我们是无法直接得到模型的。好在求解RL问题不一定需要模型，因为最终的解是策略，而不是模型。所以，可以使用无模型（Model-Free）的方法。常见的就是Monte-Carlo和Temporal Difference。这些方法都是通过样本，预估长期奖励，并且通过GPI过程逐步优化策略。（第4,5讲）

此时无模型的问题虽然解决，但是动作值函数用表格表示，无法解决连续状态问题。好在有监督学习，可以通过状态的特征，泛化出监督学习模型，得到对应的奖励。并且基于梯度的监督学习方法可以很好的与强化学习过程兼容。再者，深度学习也可以基于梯度的方法求解，并且可以自动提取特征，比如图像和时序，所以与强化学习集合产生了DQN。（第6讲）

虽然连续状态问题解决了，但是没有解决连续动作的问题。介绍的强化学习解法的另外一个方向，Policy Gradient算法。该算法直接求解策略$\pi$，而不是根据值函数间接学习策略$\pi$。最近几年新的RL框架也是基于PG的思想，比如A3C，DDPG等。（第7讲）

既然已经学习了无模型和有模型的强化学习方法，那么可以集成这两种方法，智能体一边与环境交互，一边优化模型，这样可以有效利用现有的交互，提高交互数据的利用效率。因为有时候，与环境交互的成本是很高，但是与模型的交互成本很低。（第8讲）

智能体在探索环境时，会遇到一个哲学问题---探索还是利用。需要结合当前的环境和历史数据，动态的调整探索和利用的比例，达到整体收益最大化。这些策略可以用在RL过程中的动作选择上。（第9讲）

最后，介绍了RL在游戏里面的应用，主要涉及RL结合博弈论的方法，以达到纳什均衡。由于与工作相关性不太大，所以没有仔细看。（第10讲）

经过10讲的学习，强化学习的基本理论已经理解的差不多了。现在已经满级，是时候离开新手村开始新的冒险（蜜汁微笑）！。后面还有很多Boss等待我们的挑战，比如

* 线上环境如何模拟，以及如何评估其误差？
* 离线评估的误差如何估计。
* 如何实现高并发强化学习算法？
* 多维度纬度连续动作多动作强化学习落地应用
* ...

挑战虽然艰辛，但是总归是有方法去解决，让我们拭目以待。
